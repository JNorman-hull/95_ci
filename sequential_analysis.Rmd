---
title: "Fixed-Width Sequential Confidence Interval Testing"
author: "Josh Norman"
date: "2025-07-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 6,
  fig.align = "center"
)
knitr::opts_knit$set(root.dir = getwd())
```

# 1. Introduction

This analysis aims to produce a viable Fixed-Width Sequential Confidence Interval Testing method for designing live fish trials with defined points to accept or reject null hypothesis and continue testing.

```{r test_output, eval = FALSE, include=FALSE}

test <- tibble(
  sample = 1:10,
  n_mortality = 1:10
)

write.csv(test, "./output/test.csv", row.names = FALSE)

```

```{r test_input, eval = FALSE, include=FALSE}

test_data  <- read.csv("./output/test.csv")

# 1. Filter rows where sample == 1
filter_sample1 <- test_data %>% filter(sample == 1)

# 2. Filter rows where sample == 1 AND n_mortality == 1
filter_sample1_mort1 <- test_data %>% filter(sample == 1, n_mortality == 1)

# 3. Filter rows where n_mortality != 10
filter_mort_not10 <- test_data %>% filter(n_mortality != 10)

# 4. Filter rows where sample is in a list (1, 3, 5)
filter_sample_in <- test_data %>% filter(sample %in% c(1, 3, 5))

# 5. Filter for rows with MAX n_mortality
filter_max_mort <- test_data %>% filter(n_mortality == max(n_mortality, na.rm = TRUE))

# 6. Filter for rows with MIN sample
filter_min_sample <- test_data %>% filter(sample == min(sample))

# 7. Filter rows where n_mortality > 5
filter_mort_gt5 <- test_data %>% filter(n_mortality > 5)

# 8. Filter rows where sample is between 3 and 7 (inclusive)
filter_sample_between <- test_data %>% filter(between(sample, 3, 7))

# 9. Filter rows with NA in n_mortality
filter_na_mort <- test_data %>% filter(is.na(n_mortality))

# 10. Filter rows where n_mortality is NOT NA
filter_non_na_mort <- test_data %>% filter(!is.na(n_mortality))

```

# 2. Required libraries

```{r libraries, echo=FALSE}
#library(here)
library(tidyverse)
library(binom)
library(scales)
library(plotly)
library(gganimate)
#library(ggpubr)
#library(gt)
#library(kableExtra)

#Source functions for calculations, plots etc
source("./functions.R")

```

# 3. Import

Import mortality simulation data generated in previous session.

Current data:

Desired survival: 98%
Lower bounds acceptance: 96%

Simulation details:
A true survival rate of 90 - 100% in 1% intervals
A recapture rate of 90 - 100% in 1% intervals. 

Modify survival probability to account for recapture rate

Method 1: Extrapolation
Assume that missing fish have the same survival probability as recaptured fish.
- This method is safe as we do not have sufficient information to assume missing fish are non-random. This assumes lost fish did not experience systematically different turbine/pump passage (e.g., weaker fish more likely to be lost through harms stronger fish don't experience, or, missing fish were stronger and avoided turbine passage/net entry). We cannot measure these traits within the scope of a survival study. We already grade out fish we deem to have weak behavior or injuries which could impair passage.

Instead, we assume that missing fish would've experienced the same survival probability as recaptured fish. This way, our adjusted survival probability is the same, but our confidence is reduced to account for recapture rate. Under proportional assumption, recapture rate doesn't bias survival estimates. It only affects statistical power and trial length. This is actually a strength of the proportional method. No systematic bias, just increased uncertainty.

Worked example

Exposed fish 400
Recaptured fish 360
Observed death (recaptured) 7
Observed survival rate 353/360 = 98.05% (round up to 98.06)

Missing fish 40
Under proportional assumption, apply our measured survival to entire sample

Estimated total death = 400 x (1 - 0.9806) = 7.78
Round up to nearest whole fish = 8

Look up mortality table for 400, 8, instead of 400, 7.

- Adjusted = 98.25% [CI: 96.43% - 99.15%]
- Adjusted = 98% [CI: 96.1% - 98.98%]

We have reduced our confidence by 0.34% to account for recapture loss

Method 2: Maximum Likelihood Estimation

```{r input_import}

mortality_data  <- read.csv("./input/mortality_data_98surv_96lower_step1.csv")
simulation_results  <- read.csv("./input/simulation_results_98surv_96lower_10recap.csv")

lookup_mortality_data(mortality_data, 400, 8) 
lookup_mortality_data(mortality_data, 400, 7) 

```

# 4. Fixed-precision Sequential Confidence Interval Testing

Fixed-width sequential confidence interval testing is a statistical method used to estimate a parameter (e.g., mean, proportion) with a predetermined precision while dynamically adjusting the sample size based on accumulating data. Unlike fixed-sample designs, this approach continuously monitors the width of the confidence interval (CI) during data collection, stopping only once the CI narrows to a pre-specified fixed width, ensuring the estimate meets the desired margin of error. Sequential testing is particularly useful when data collection is costly or time-consuming, as it often requires fewer observations than fixed-sample methods while maintaining statistical validity.

We deviate from this principle by fixing our precision level as apposed to the width of the CI.

## 4.1 Unadjusted CIs

Ensure we calculate our confidence intervals based on the survival probability, NOT, the mortality probability. This may be a source of error for the previous differences in calculations. 

At each sampling point (e.g., every n = 100), calculate Wilson confidence intervals for observed mortality number. This will inherently deviate from fixed-width CI as the CI width is determined by observed mortality and sample size.

Decision rules to estimate probability with a given precision level (differs from hypothesis based testing):

-   Desired threshold for survival ≥ 98%

-   Not fish-friendly : If upper 95% CI \< 98%

-   Fish-friendly: If observed survival ≥ 98% and lower 95% CI ≥ 94%

-   Continue testing: Otherwise

The continue zone therefore directly controls the estimation error and we do not apply traditional hypothesis based testing.\
\
**Accepting only when results are clear and conservative (\>98% + LB ≥ 94%). Rejecting only when results are definitively bad (UB \< 98%). Continuing otherwise—no gray-area decisions.**

**Type I/II Error Control**

-   **Type I Error (False "Accept"):**

    -   Controlled by requiring **LB ≥ 94% AND observed \> 98%**.

    -   This is stricter than classic hypothesis tests (reduces false positives).

-   **Type II Error (False "Reject"):**

    -   Controlled by **only rejecting if UB \< 98%** (avoids premature rejection)

**Conservatism**

-   Sequential testing is required in our application as we do not have sufficient knowledge of the actual mortality rate to apply a fixed sample size. 

-   Our first sample occurs at 100, which has already significantly reduced our likelihood of committing a type I error. We simple maintain our artificial alpha spend linearly throughout the experiment. Bare in mind there is no true alpha when we are just examining within group probability and want a fixed precision. That will come in the next approach of comparing impact to control groups.

-   The criteria for rejecting the null are already high by requiring an upper bounds to not include 98.

-   Fixed lower bound rule is a conservative measure to guarantee fixed estimation lower bounds.

Because this restriction is already conservative, we choose not to apply any further adjustment to the confidence intervals.

#### 4.1.1 Build mortality table

For borderline cases, check the threshold against mortality table to establish exact criteria 

```{r mortality_table}

# mortality_data <- create_mortality_table(
#   max_sample_size = 1000,
#   step_size = 1,
#   lower_threshold = 96,
#   desired_surv = 98,
#   max_mortality_proportion = 0.5
# )

#using step size of 1 creates mortality table robust to any recapture rate 

#write.csv(mortality_data, "./output/mortality_data.csv", row.names = FALSE)

mortality_sample_plot <- create_decision_plot(mortality_data)
mortality_sample_plot

#ggsave(filename = "./output/mortality.svg", plot = mortality_sample_plot,
#      units="cm",width=14,height=14)

```

```{r plotly, echo=FALSE, eval = FALSE}

# Create reduced step table for interactive plot
mortality_data2 <- create_mortality_table(
  max_sample_size = 1000,
  step_size = 10,
  lower_threshold = 96,
  desired_surv = 98,
  max_mortality_proportion = 0.5
)

mortality_sample_plot2 <- create_decision_plot(mortality_data2)

decision_point <- mortality_sample_plot2 +
  geom_point(data = mortality_data2, 
             aes(x = sample_size, y = n_mortality, 
                 text = survival_formatted, color = decision),
             alpha = 0, size = 1)

decision_point_plotly <- ggplotly(decision_point, tooltip = "text")
decision_point_plotly

```

### 4.1.2 Simulating sequntial choice scenarios

We must validate our approach to sequentially testing survival under different true mortality scenarios.
This demonstrates the practical implications for trial designs.

```{r}

# execution_time <- system.time(
#   simulation_results <- simulate_sequential_trial(
#     simulations = 1:1000,
#     max_sample = 1000,
#     true_survival_rates = round(seq(0.90, 1, 0.01), 2),  
#     check_intervals = c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000), 
#     mortality_table = mortality_data,
#     recapture_rate = round(seq(0.90, 1, 0.01), 2)
#   )
# )
# 
# print(execution_time)
# 
# write.csv(simulation_results, "./output/simulation_results.csv", row.names = FALSE)
#simulation takes 1765 seconds, ~30 minutes


#ggsave(filename = "./output/mortality_journey_97.svg", plot = journey_plot_20,
 #     units="cm",width=22,height=16)


journey_plot_single <- create_journey_plot(mortality_data, simulation_results, 
                                           survival_rates_to_show = c(0.98),
                                           recapture_rate_to_show = 1.0,
                                           n_journeys = 1000)

journey_plot_single + theme(legend.position = "none")

#this allows you to say "if the True survival is >98%, then we will accept by interval 3 (total n = 300) 


```



```{r eval = FALSE}

anim_plot <- create_animated_journey_plot(mortality_data, simulation_results,
                                          survival_rates_to_show = c(0.98),
                                          recapture_rate_to_show = 1.0,
                                          n_journeys = 3)

# Render animation
animated_gif <- animate(anim_plot, 
                       width = 800, height = 600, 
                       duration = 8,  # 8 seconds total
                       fps = 30,      # 10 frames per second
                       renderer = gifski_renderer("journey_animation.gif"))

# Display
animated_gif

```


```{r eval= FALSE}


# Generate summaries
summaries <- create_stopping_summary(simulation_results)

# View stopping details
summaries$stopping_details

# View trial overview  
summaries$trial_overview

```


```{r eval = FALSE}
# Determine whether recapture rate influences stopping decisions 
# may be useless, not sure if I'm measuring the final decision, or proportion of all decision points through trial

decision_prop <- plot_decision_proportions(simulation_results)
decision_prop

# Determine the sample size at which trials are usually stopped, given by recapture rate and true survival
stopping_prop <- plot_stopping_distributions(simulation_results)
stopping_prop

# Visualisation not particularly helpful, should tabulate and measure differences in proportions
```


```{r}

cumulative_data <- calculate_cumulative_stopping(simulation_results)

# View specific scenario
cumulative_data %>%
  filter(true_survival_rate == 0.95, recapture_rate == 1.0) %>%
  select(interval, prop_accept_by_now, prop_reject_by_now, prop_continue) 


cumulative_plot <- plot_cumulative_stopping(cumulative_data, 
                                           survival_filter = 0.98, 
                                           recapture_filter = 1)

cumulative_plot <- plot_cumulative_stopping(cumulative_data)


ggsave(filename = "./output/cumulative_plot.svg", plot = cumulative_plot,
      units="cm",width=14,height=14)

inconclusiveness_wide <- create_inconclusiveness_wide(cumulative_data)
print(inconclusiveness_wide)


```

## 4.2 Adjusted CIs

Alternatively, we could do Group-Sequential Design with Error-Spending

We could use O'Brien-Fleming alpha spending curve IF we are willing to accept a wider confidence interval but want to prevent early acceptance which only represents random probability in the population e.g., you may need a sample size bigger than your first stop point if the effect is small. Pocock could be a useful method if we want to be more conservative towards rejection, as it is sensitive for extreme cases (0, 1), but requires increasing confidence to reject the null hypothesis e.g., saying a pump isn't fish-friendly, will require slightly more evidence

# 5 Control group comparison

When mortality is rare (e.g., 2%), the absolute survival rate tells you that it's high, but not whether the system caused harm.

Example:
Pump group: 98% survival (2% mortality)

Is that 2% due to the pump? Or would we have seen it anyway?

Without a control group (e.g., fish handled but not pumped), you can't tell if:

That 2% is normal background mortality (e.g., stress, capture)

Or whether the pump adds additional risk

When mortality occurs and we want to be certain the mortality would not occur if fish were not exposed to the pump, then we require statistical comparisons to a control group population.

Unbalanced study design as large control group for small effect sizes undesired

Use a ratio of 3:1 or 4:1 (justified by literature)

test options

z-test proportions
fishers exact (where control mortality is 0)
GLMM (where mixed effects are desired)


Use power of x, alpha of x and effect of x

User can change these as desired

Do a one-sided test as only looking to see if mortality in the control group is different to mortality in the impact group

Pick the correct test for this type of data

then build a SPRT were we determine likelihood of a significant test result to make decisions whilst testing fish

Have correction in this for type 1 and 2 error

Then simulate scenarios

Do you always need high precision within group if we are also measuring comparison to control?
