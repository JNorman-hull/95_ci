---
title: "Fixed-Width Sequential Confidence Interval Testing"
author: "Josh Norman"
date: "2025-07-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 6,
  fig.align = "center"
)
knitr::opts_knit$set(root.dir = getwd())
```

# 1. Introduction

This analysis aims to produce a viable Fixed-Width Sequential Confidence Interval Testing method for designing live fish trials with defined points to accept or reject null hypothesis and continue testing.

```{r test_output, eval = FALSE, include=FALSE}

test <- tibble(
  sample = 1:10,
  n_mortality = 1:10
)

write.csv(test, "./output/test.csv", row.names = FALSE)

```

```{r test_input, eval = FALSE, include=FALSE}

test_data  <- read.csv("./output/test.csv")

# 1. Filter rows where sample == 1
filter_sample1 <- test_data %>% filter(sample == 1)

# 2. Filter rows where sample == 1 AND n_mortality == 1
filter_sample1_mort1 <- test_data %>% filter(sample == 1, n_mortality == 1)

# 3. Filter rows where n_mortality != 10
filter_mort_not10 <- test_data %>% filter(n_mortality != 10)

# 4. Filter rows where sample is in a list (1, 3, 5)
filter_sample_in <- test_data %>% filter(sample %in% c(1, 3, 5))

# 5. Filter for rows with MAX n_mortality
filter_max_mort <- test_data %>% filter(n_mortality == max(n_mortality, na.rm = TRUE))

# 6. Filter for rows with MIN sample
filter_min_sample <- test_data %>% filter(sample == min(sample))

# 7. Filter rows where n_mortality > 5
filter_mort_gt5 <- test_data %>% filter(n_mortality > 5)

# 8. Filter rows where sample is between 3 and 7 (inclusive)
filter_sample_between <- test_data %>% filter(between(sample, 3, 7))

# 9. Filter rows with NA in n_mortality
filter_na_mort <- test_data %>% filter(is.na(n_mortality))

# 10. Filter rows where n_mortality is NOT NA
filter_non_na_mort <- test_data %>% filter(!is.na(n_mortality))

```

# 2. Required libraries

```{r libraries, echo=FALSE}
#library(here)
library(tidyverse)
library(binom)
library(scales)
library(plotly)
#library(ggpubr)
#library(gt)
#library(kableExtra)

#Source functions for calculations, plots etc
source("./functions.R")

```

# 3. Helper functions

# 4. Fixed-precision Sequential Confidence Interval Testing

Fixed-width sequential confidence interval testing is a statistical method used to estimate a parameter (e.g., mean, proportion) with a predetermined precision while dynamically adjusting the sample size based on accumulating data. Unlike fixed-sample designs, this approach continuously monitors the width of the confidence interval (CI) during data collection, stopping only once the CI narrows to a pre-specified fixed width, ensuring the estimate meets the desired margin of error. Sequential testing is particularly useful when data collection is costly or time-consuming, as it often requires fewer observations than fixed-sample methods while maintaining statistical validity.

We deviate from this principle by fixing our precision level as apposed to the width of the CI.

## 4.1 Unadjusted CIs

Ensure we calculate our confidence intervals based on the survival probability, NOT, the mortality probability. This may be a source of error for the previous differences in calculations. 

At each sampling point (e.g., every n = 100), calculate Wilson confidence intervals for observed mortality number. This will inherently deviate from fixed-width CI as the CI width is determined by observed mortality and sample size.

Decision rules to estimate probability with a given precision level (differs from hypothesis based testing):

-   Desired threshold for survival ≥ 98%

-   Not fish-friendly : If upper 95% CI \< 98%

-   Fish-friendly: If observed survival ≥ 98% and lower 95% CI ≥ 94%

-   Continue testing: Otherwise

The continue zone therefore directly controls the estimation error and we do not apply traditional hypothesis based testing.\
\
**Accepting only when results are clear and conservative (\>98% + LB ≥ 94%). Rejecting only when results are definitively bad (UB \< 98%). Continuing otherwise—no gray-area decisions.**

**Type I/II Error Control**

-   **Type I Error (False "Accept"):**

    -   Controlled by requiring **LB ≥ 94% AND observed \> 98%**.

    -   This is stricter than classic hypothesis tests (reduces false positives).

-   **Type II Error (False "Reject"):**

    -   Controlled by **only rejecting if UB \< 98%** (avoids premature rejection)

**Conservatism**

-   Sequential testing is required in our application as we do not have sufficient knowledge of the actual mortality rate to apply a fixed sample size. 

-   Our first sample occurs at 100, which has already significantly reduced our likelihood of committing a type I error. We simple maintain our artificial alpha spend linearly throughout the experiment. Bare in mind there is no true alpha when we are just examining within group probability and want a fixed precision. That will come in the next approach of comparing impact to control groups.

-   The criteria for rejecting the null are already high by requiring an upper bounds to not include 98.

-   Fixed lower bound rule is a conservative measure to guarantee fixed estimation lower bounds.

Because this restriction is already conservative, we choose not to apply any further adjustment to the confidence intervals.

#### 4.1.1 Build mortality table

For borderline cases, check the threshold against mortality table to establish exact criteria 

```{r mortality_table}

mortality_data <- create_mortality_table(
  max_sample_size = 1000,
  step_size = 10,
  lower_threshold = 96,
  desired_surv = 98,
  max_mortality_proportion = 1
)

#write.csv(mortality_data, "./output/mortality_data.csv", row.names = FALSE)

mortality_sample_plot <- create_decision_plot(mortality_data)
mortality_sample_plot

ggsave(filename = "./output/mortality.svg", plot = mortality_sample_plot,
      units="cm",width=14,height=14)

```

```{r plotly, echo=FALSE, eval = FALSE}


decision_point <- mortality_sample_plot +
  geom_point(data = mortality_data, 
             aes(x = sample_size, y = n_mortality, 
                 text = survival_formatted, color = decision),
             alpha = 0, size = 1)

decision_point_plotly <- ggplotly(decision_point, tooltip = "text")

decision_point_plotly

```

### 4.1.2 Simulating sequntial choice scenarios

We must validate our approach to sequentially testing survival under different true mortality scenarios.
This demonstrates the practical implications for trial designs.

```{r}

simulation_results <- simulate_sequential_trial(
  simulations = 1:1000,
  max_sample = 1000,
  true_survival_rates = 0.98,  # 90% to 100% in 1% steps
  check_intervals = c(100, 200, 300, 400, 500, 600, 700, 800, 900, 1000), 
  mortality_table = mortality_data
)

#write.csv(simulation_results, "./output/simulation_results.csv", row.names = FALSE)


# Plot journeys for 98% survival
journey_plot_5 <- create_journey_plot(mortality_data, simulation_results, 
                                      survival_rate_to_show = 0.98, 
                                      n_journeys = 5)
journey_plot_5
journey_plot_20
# Show 20 journeys
journey_plot_20 <- create_journey_plot(mortality_data, simulation_results, 
                                       survival_rate_to_show = 0.98, 
                                       n_journeys = 500)

# Show specific journey IDs
journey_plot_specific <- create_journey_plot(mortality_data, simulation_results, 
                                             survival_rate_to_show = 0.98, 
                                             journey_ids = c(1, 5, 10, 15, 20))



```


## 4.2 Adjusted CIs

Alternatively, we could do Group-Sequential Design with Error-Spending

We could use O'Brien-Fleming alpha spending curve IF we are willing to accept a wider confidence interval but want to prevent early acceptance which only represents random probability in the population e.g., you may need a sample size bigger than your first stop point if the effect is small. Pocock could be a useful method if we want to be more conservative towards rejection, as it is sensitive for extreme cases (0, 1), but requires increasing confidence to reject the null hypothesis e.g., saying a pump isn't fish-friendly, will require slightly more evidence

# 5 Control group comparison

When mortality is rare (e.g., 2%), the absolute survival rate tells you that it's high, but not whether the system caused harm.

Example:
Pump group: 98% survival (2% mortality)

Is that 2% due to the pump? Or would we have seen it anyway?

Without a control group (e.g., fish handled but not pumped), you can't tell if:

That 2% is normal background mortality (e.g., stress, capture)

Or whether the pump adds additional risk

When mortality occurs and we want to be certain the mortality would not occur if fish were not exposed to the pump, then we require statistical comparisons to a control group population.

Unbalanced study design as large control group for small effect sizes undesired

Use a ratio of 3:1 or 4:1 (justified by literature)

test options

z-test proportions
fishers exact (where control mortality is 0)
GLMM (where mixed effects are desired)


Use power of x, alpha of x and effect of x

User can change these as desired

Do a one-sided test as only looking to see if mortality in the control group is different to mortality in the impact group

Pick the correct test for this type of data

then build a SPRT were we determine likelihood of a significant test result to make decisions whilst testing fish

Have correction in this for type 1 and 2 error

Then simulate scenarios

Do you always need high precision within group if we are also measuring comparison to control?
