---
title: "Fixed-Width Sequential Confidence Interval Testing"
author: "Josh Norman"
date: "2025-07-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 6,
  fig.align = "center"
)
knitr::opts_knit$set(root.dir = getwd())
```

# 1. Introduction

This analysis aims to produce a viable Fixed-Width Sequential Confidence Interval Testing method for designing live fish trials with defined points to accept or reject null hypothesis and continue testing.

```{r test_output, eval = FALSE, include=FALSE}

test <- tibble(
  sample = 1:10,
  n_mortality = 1:10
)

write.csv(test, "./output/test.csv", row.names = FALSE)

```

```{r test_input, eval = FALSE, include=FALSE}

test_data  <- read.csv("./output/test.csv")

# 1. Filter rows where sample == 1
filter_sample1 <- test_data %>% filter(sample == 1)

# 2. Filter rows where sample == 1 AND n_mortality == 1
filter_sample1_mort1 <- test_data %>% filter(sample == 1, n_mortality == 1)

# 3. Filter rows where n_mortality != 10
filter_mort_not10 <- test_data %>% filter(n_mortality != 10)

# 4. Filter rows where sample is in a list (1, 3, 5)
filter_sample_in <- test_data %>% filter(sample %in% c(1, 3, 5))

# 5. Filter for rows with MAX n_mortality
filter_max_mort <- test_data %>% filter(n_mortality == max(n_mortality, na.rm = TRUE))

# 6. Filter for rows with MIN sample
filter_min_sample <- test_data %>% filter(sample == min(sample))

# 7. Filter rows where n_mortality > 5
filter_mort_gt5 <- test_data %>% filter(n_mortality > 5)

# 8. Filter rows where sample is between 3 and 7 (inclusive)
filter_sample_between <- test_data %>% filter(between(sample, 3, 7))

# 9. Filter rows with NA in n_mortality
filter_na_mort <- test_data %>% filter(is.na(n_mortality))

# 10. Filter rows where n_mortality is NOT NA
filter_non_na_mort <- test_data %>% filter(!is.na(n_mortality))

```

# 2. Required libraries

```{r libraries, echo=FALSE}
#library(here)
library(tidyverse)
library(binom)
library(scales)
#library(ggpubr)
#library(gt)
#library(kableExtra)

```

# 3. Helper functions

```{r theme_jn, echo= FALSE, include = FALSE}

theme_JN <- function(base_size=10){ 
  theme_grey() %+replace%
    theme(
      axis.text = element_text(colour="black"),
      axis.title = element_text(colour="black"),
      axis.ticks = element_line(colour="black"),
      panel.border = element_rect(colour = "black", fill=NA),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      panel.background = element_blank(),
      strip.background = element_rect(colour = "black",fill = NA),
      panel.spacing.x = unit(12, "pt")
    ) 
}


```

### create_mortality_table

Function to calculate 95% Wilson Confidence Intervals for each mortality at a given sample size

```{r create_mortality_table, echo= FALSE, include = FALSE}
 create_mortality_table<- function(
    max_sample_size = 1000, 
    step_size = 50,
    lower_threshold = 94,
    desired_surv = 98,
    max_mortality_proportion = 0.10) {
   
    if (lower_threshold > desired_surv) {
    stop("Error: `lower_threshold` (", lower_threshold, 
         ") cannot be greater than `desired_surv` (", desired_surv, ")")
  }
  
  sample_sizes <- seq(0, max_sample_size, by = step_size)
  
  do.call(rbind, lapply(sample_sizes, function(n) {
    n_mortality <- 0:floor(max_mortality_proportion * n)
    
    if (n == 0) {
      percent <- rep(0, length(n_mortality))
      lower <- rep(0, length(n_mortality))
      upper <- rep(0, length(n_mortality))
    } else {
      conf <- binom.confint(n_mortality, n, methods = "wilson")
      percent <- round(conf$mean * 100, 2)
      lower <- round(conf$lower * 100, 2)
      upper <- round(conf$upper * 100, 2)
    }
    
    survival <- round(100 - percent, 2)
    ci_lower_surv <- round(100 - upper, 2)
    ci_upper_surv <- round(100 - lower, 2)
    
  
    
     decision <- case_when(
      survival >= desired_surv & ci_lower_surv >= lower_threshold ~ "Accept H0",
      ci_upper_surv < desired_surv + 0.1 ~ "Reject H0",
      TRUE ~ "Continue"
    )
    
    data.frame(
      sample_size = n,
      n_mortality = n_mortality,
      survival = survival,
      ci_lower_surv = ci_lower_surv,
      ci_upper_surv = ci_upper_surv,
      decision = factor(decision, levels = c("Accept H0", "Reject H0", "Continue"))
    )
  }))
}

```

### create_decision_plot

```{r create_decision_plot}

# Complete modified function with smooth boundary lines
create_decision_plot <- function(mortality_table, desired_surv = 98, lower_threshold = 94) {
  # Calculate dynamic axis limits
  x_limits <- c(
    min(mortality_table$sample_size[mortality_table$sample_size > 0], na.rm = TRUE),
    max(mortality_table$sample_size, na.rm = TRUE)
  )
  
  y_max <- mortality_table %>% 
    filter(sample_size == max(sample_size), decision == "Continue") %>%
    pull(n_mortality) %>%
    max(na.rm = TRUE)
  
  y_limits <- c(0, y_max * 1.05)
  
  # Create boundary points that extend to plot edges
  continue_points <- mortality_table %>%
    filter(sample_size > 0, decision == "Continue")
  
  # Create base plot without points
  p <- ggplot() +
    xlim(x_limits) +
    ylim(y_limits)
  
  # Add filled regions if we have Continue points
  if(nrow(continue_points) > 0) {
    # Find actual boundaries
    boundaries_exact <- continue_points %>%
      group_by(sample_size) %>%
      summarise(
        max_continue = max(n_mortality),
        min_continue = min(n_mortality),
        .groups = "drop"
      )
    
    # Create boundary points with minimal buffer and extend well beyond plot edges
    boundary_data <- boundaries_exact %>%
      mutate(
        upper_boundary = max_continue + 0.7,
        lower_boundary = pmax(0, min_continue - 0.7)
      ) %>%
      # Add points well beyond plot edges to ensure full coverage
      bind_rows(
        data.frame(
          sample_size = c(x_limits[1] - 50, x_limits[2] + 50),
          max_continue = NA,
          min_continue = NA,
          upper_boundary = c(first(.$upper_boundary), last(.$upper_boundary)),
          lower_boundary = c(first(.$lower_boundary), last(.$lower_boundary))
        )
      ) %>%
      arrange(sample_size)
    
    # Create smooth boundary predictions for filling
    if(nrow(boundary_data) > 3) {
      # Fit smooth models to get predictions across extended x range
      x_seq <- seq(x_limits[1] - 10, x_limits[2] + 10, length.out = 150)
      
      # Fit and predict upper boundary
      upper_model <- loess(upper_boundary ~ sample_size, data = boundary_data, span = 0.75)
      upper_pred <- predict(upper_model, newdata = data.frame(sample_size = x_seq))
      
      # Fit and predict lower boundary  
      lower_model <- loess(lower_boundary ~ sample_size, data = boundary_data, span = 0.75)
      lower_pred <- predict(lower_model, newdata = data.frame(sample_size = x_seq))
      
      # Ensure predictions don't go below 0
      lower_pred <- pmax(0, lower_pred)
      
      # Create data frame for ribbons
      ribbon_data <- data.frame(
        x = x_seq,
        upper = upper_pred,
        lower = lower_pred
      )
      
      # Add filled regions
      # Reject region (above upper boundary)
      p <- p + geom_ribbon(data = ribbon_data,
                          aes(x = x, ymin = upper, ymax = y_limits[2]),
                          fill = "red", alpha = 0.2)
      
      # Continue region (between boundaries)
      p <- p + geom_ribbon(data = ribbon_data,
                          aes(x = x, ymin = lower, ymax = upper),
                          fill = "grey", alpha = 0.2)
      
      # Accept region (below lower boundary)
      p <- p + geom_ribbon(data = ribbon_data,
                          aes(x = x, ymin = 0, ymax = lower),
                          fill = "green", alpha = 0.2)
      
      # Add boundary lines
      p <- p + geom_line(data = ribbon_data,
                        aes(x = x, y = upper),
                        color = "black")
      
      p <- p + geom_line(data = ribbon_data,
                        aes(x = x, y = lower),
                        color = "black")
    }
  }
  
 p <- p +
    annotate("text", x = x_limits[2] * 0.25, y = y_limits[2] * 0.85, 
             label = "Reject H0: Not fish-friendly", size = 4) +
    annotate("text", x = mean(x_limits), y = mean(y_limits), 
             label = "Precision not achieved: Continue testing", size = 4) +
    annotate("text", x = x_limits[2] * 0.75, y = y_limits[2] * 0.15, 
             label = "Accept H0: Fish-friendly", size = 4)
  
  # Add dynamic parameter labels
  param_text <- paste0("CI = 95% (Wilson Binomial)\n",
                      "Desired survival ≥ ", desired_surv, "%\n",
                      "Lower bound threshold ≥ ", lower_threshold, "%")
  
  p <- p +
    annotate("text", x = x_limits[2] * 0.98, y = y_limits[2] * 0.98, 
             label = param_text, size = 4, hjust = 1, vjust = 1,
             fontface = "italic")
  
  # Add styling
  p <- p +
    scale_y_continuous(
      limits = y_limits,
      breaks = pretty_breaks(),
      expand = c(0, 0)
    ) +
    scale_x_continuous(
      limits = x_limits,
      breaks = pretty_breaks(),
      expand = c(0, 0)
    ) +
    labs(
      x = "Sample Size",
      y = "Mortality (n)"
    ) +
    theme_JN() +
    theme(
      panel.grid.major = element_line(color = "white", size = 0.5),
      panel.grid.minor = element_line(color = "white", size = 0.3)
    )+
    coord_cartesian(clip = "off")
  
  return(p)
}

```


# 4. Fixed-precision Sequential Confidence Interval Testing

Fixed-width sequential confidence interval testing is a statistical method used to estimate a parameter (e.g., mean, proportion) with a predetermined precision while dynamically adjusting the sample size based on accumulating data. Unlike fixed-sample designs, this approach continuously monitors the width of the confidence interval (CI) during data collection, stopping only once the CI narrows to a pre-specified fixed width, ensuring the estimate meets the desired margin of error. Sequential testing is particularly useful when data collection is costly or time-consuming, as it often requires fewer observations than fixed-sample methods while maintaining statistical validity.

We deviate from this principle by fixing our precision level as apposed to the width of the CI.

## 4.1 Unadjusted CIs

At each sampling point (e.g., every n = 100), calculate Wilson confidence intervals for observed mortality number. This will inherently deviate from fixed-width CI as the CI width is determined by observed mortality and sample size.

Decision rules to estimate probability with a given precision level (differs from hypothesis based testing):

-   Desired threshold for survival ≥ 98%

-   Not fish-friendly : If upper 95% CI \< 98%

-   Fish-friendly: If observed survival ≥ 98% and lower 95% CI ≥ 94%

-   Continue testing: Otherwise

The continue zone therefore directly controls the estimation error and we do not apply traditional hypothesis based testing.\
\
**Accepting only when results are clear and conservative (\>98% + LB ≥ 94%). Rejecting only when results are definitively bad (UB \< 98%). Continuing otherwise—no gray-area decisions.**

**Type I/II Error Control**

-   **Type I Error (False "Accept"):**

    -   Controlled by requiring **LB ≥ 94% AND observed \> 98%**.

    -   This is stricter than classic hypothesis tests (reduces false positives).

-   **Type II Error (False "Reject"):**

    -   Controlled by **only rejecting if UB \< 98%** (avoids premature rejection)

**Conservatism**

-   Sequential testing is required in our application as we do not have sufficient knowledge of the actual mortality rate to apply a fixed sample size. 

-   Our first sample occurs at 100, which has already significantly reduced our likelihood of committing a type I error. We simple maintain our artificial alpha spend linearly throughout the experiment. Bare in mind there is no true alpha when we are just examining within group probability and want a fixed precision. That will come in the next approach of comparing impact to control groups.

-   The criteria for rejecting the null are already high by requiring an upper bounds to not include 98.

-   Fixed lower bound rule is a conservative measure to guarantee fixed estimation lower bounds.

Because this restriction is already conservative, we choose not to apply any further adjustment to the confidence intervals.

#### 4.1.1 Build mortality table

```{r mortality_table}

mortality_data <- create_mortality_table(
  max_sample_size = 100,
  step_size = 10,
  lower_threshold = 94,
  desired_surv = 98,
  max_mortality_proportion = 1
)

#This needs to be saving desird surv and lower threshold in the tibble

mortality_sample_plot <- create_decision_plot(mortality_data)
mortality_sample_plot

#ggsave(filename = "./output/mortality.svg", plot = mortality_sample_plot,
#       units="cm",width=12,height=10)

```

## 4.2 Adjusted CIs

Alternatively, we could do Group-Sequential Design with Error-Spending

We could use O'Brien-Fleming alpha spending curve IF we are willing to accept a wider confidence interval but want to prevent early acceptance which only represents random probability in the population e.g., you may need a sample size bigger than your first stop point if the effect is small. Pocock could be a useful method if we want to be more conservative towards rejection, as it is sensitive for extreme cases (0, 1), but requires increasing confidence to reject the null hypothesis e.g., saying a pump isn't fish-friendly, will require slightly more evidence

# 5 Control group comparison

When mortality occurs and we want to be certain the mortality would not occur if fish were not exposed to the pump, then we require statistical comparisons to a control group population.

Unbalanced study design as large control group for small effect sizes undesired

Use power of x, alpha of x and effect of x

User can change these as desired

Do a one-sided test as only looking to see if mortality in the control group is different to mortality in the impact group

Pick the correct test for this type of data

then build a SPRT were we determine likelihood of a significant test result to make decisions whilst testing fish

Have correction in this for type 1 and 2 error

Then simulate scenarios

Do you always need high precision within group if we are also measuring comparison to control?
